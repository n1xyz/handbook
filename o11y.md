# Observability

Observability is difficult to do after the fact. Be thorough. The
heuristic to use is:

> Looking at the dashboards, which line of code my program is at?

We all hate debugging, let's make it trivial. If shit hits the fan, we
know exactly what happened and can promptly fix it. This doesn't just
come by itself, but with thoughtful design and incorporation of
observability every step of the way. To start, I recommend:

- [All you need is Wide Events, not "Metrics, Logs and
  Traces"](https://isburmistrov.substack.com/p/all-you-need-is-wide-events-not-metrics)

In our case, we conceptually capture events as structured logs, and
metrics for anything that fits better as a metric. Specifically:

- [`tracing`](https://docs.rs/tracing/latest/tracing/) for emitting
  _events_ as structured logs.

  ```rust
  let result = execute(&action);

  tracing::info!(
      ?action,
      ?result,
      "executed",
  );
  ```

  Try to use a simple static string as the event name, preferably
  lowercase. Usually, the span the event is part of and the line and
  file numbers will give enough context on _where_ the event happened,
  the event name should answer the _what_, succinctly. Keep in mind that
  usually the person reading the event will not be familiar with your
  codebase, so keep it clear.

  Some events may be part of a _trace_ which is a sequence of events.
  Traces are great for capturing the flow of an operation as it
  traverses the system. Take the time to familiarize yourself with how
  the crate works.

  Make sure to leverage its traces to capture _context_, which makes a
  world of difference when debugging long running flow. With traces,
  sequential events can be grouped together, which is critical for
  debugging highly concurrent systems.

  ```rust
  #[tracing::instrument(
      skip_all,
      err,
      ret(level = "debug"),
      fields(value = %value),
  )]
  async fn foo(&self, value: u64) -> Result<u64> {
      // ...
      // any event here and future scopes will be part of the trace
      // ...
  }
  ```

  Note that there are many ways to capture context beyond `instrument`.
  In a pipelined architecture, the `tracing::Span` can be passed around.

  ```rust
  let cx = RequestContext {
      payload: Bytes,
      span: tracing::info_span!("foo");
  };

  let _guard = cx.span.enter();
  tracing::trace!(?payload, "got payload");
  ```

- [`metrics`](https://docs.rs/metrics/latest/metrics/) for capturing
  measurements of the system and counters for events that would be too
  repetitive as events.

  ```rust
  let exec_time = metrics::histogram!("exec_time");

  loop {
      let action = rx.recv()?;
      let t = Instant::now();
      let result = execute(&action);
      exec_time.record(t.elapsed().as_secs_f64());
  }
  ```

  We don't have an explicit naming convention, but recommend roughly
  following standard guidelines from Prometheus:

  - [Metric and label naming](https://prometheus.io/docs/practices/naming/)
  - [On the naming of things](https://www.robustperception.io/on-the-naming-of-things/)

Both of these crates provide minimal and simple frontends with pluggable
backends, making them great for use in libraries. For example, we use
loki as our backend for `tracing` and prometheus as our backend for
`metrics`. However, we could naturally swap out these for something
else and likely will.

## Example metrics

Don't forget that not all metrics are for engineers. Some are great for
showing off in our marketing materials or to prospective partners. We're
quite proud of our latency numbers for example.

- execution latency for various stages

  One example is in nord, where we measure the execution time of
  components like wal writes, engine execution, and so on. This
  helps us track performance in production over time and identify
  regressions.

- request latency (success, failure, etc.)

  Request latency is probably our most important metric given the
  scope of the product.

- request count (success, failure, dropped, etc.)

  These can be used to extrapolate other metrics, such as
  total requests per second and error rates. Error rates are
  great when paired with some form of anomaly detection.

- rate limits being hit

  Can give an estimate of if we're under a bit higher load, or if
  somebody is abusing our API.

- queue lengths, depth, backlogs, wait times

  Typically, we actually use _histograms_ for queue lengths since they
  give a better picture of how queue lengths are fluctuating. Queue
  lengths are great for identifying bottlenecks.

- system metrics like cpu, memory, vmstat, etc.

  We just use the [prometheus node
  exporter](https://github.com/prometheus/node_exporter) for this, which
  is easy to set up on nixos. These stats should always be part of
  dashboards.

- wallet balance

  For any service that executes transactions on-chain, we need to ensure
  that the wallet is funded. It's critical to monitor it to avoid
  downtime.

For some extra inspiration, consider reading through [this paper on
statistics on operations data](Statistics for Engineers: Applying
statistical techniques to operations data).

## What not to do

You'll notice that we don't use `opentelemetry`. In our experience, the
rust crates are too complex and the crates above provide much simpler
and more ergonomic interfaces. The ecosystem may mature to something
more ergonomic, but we're still far from that. Don't be afraid to write
your own `metrics` and `tracing` backends to fill that gap in the
meantime; the performance will likely be better anyways.
